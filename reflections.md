I used a coding assistant to scaffold the LangGraph nodes, write GitHub API wrappers, and iterate on the LLM judge prompts. It helped me converge on a structured-output approach (Pydantic with Gemini) and catch subtle issues like JSON formatting and path styles. It was most useful for boilerplate and prompt phrasing; least useful when it suggested deprecated imports (Pydantic v1) and when it returned non-JSON outputs. I mitigated that by switching to with_structured_output() and adding a strict evidence verifier so the model’s claims must match fetched signals. It also suggested redline logic I tightened into deterministic rules the LLM can’t override. I added OSS-friendly fallbacks (treat any failed check as a redline) so reviewers can reproduce without admin rights. Overall, the assistant accelerated me, but I kept a human-in-the-loop mindset: verify imports, handle 401s by retrying unauthenticated, and fail safe to PAUSE on any schema/evidence mismatch.